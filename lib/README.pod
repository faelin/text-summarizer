=pod
 
=encoding utf-8  



=head1 NAME

Text::Summarizer - Summarize Bodies of Text  



=head1 SYNOPSIS

    use Text::Summarizer;

      # all constructor arguments shown are OPTIONAL and reflect the DEFAULT VALUES of each attribute
    $summarizer = Text::Summarizer->new(
                    articles_path  => 'directory/to/summarize/*',
                    permanent_path => 'data/permanent.stop',
                    stopwords_path => 'data/stopwrods.stop',
                    typifier       => Text::Typifier->new();
                    scanner        => Text::Summarizer::Scanner->new();
                    print_working  => 0,
                    print_scanner  => 0,
                    print_summary  => 0,
                    print_graphs   => 0,
                    print_typifier => 0,
                    return_count   => 20,
                    phrase_thresh  => 2,
                    phrase_radius  => 5,
                    freq_constant  => 0.004,
                );

      # to summarize a string
    $stopwords = $summarizer->scan_text('your text goes here');
    $summary   = $summarizer->summ_text('your text goes here');

      # or to summarize an entire file
    $stopwords = $summarizer->scan_file("/some/file.txt");
    $summary   = $summarizer->summ_file("/some/file.txt");

      # or to summarize in bulk
      #   (if no argument provided, uses the 'articles_path' attribute)
    @stopwords = $summarizer->scan_each("/directory/glob/*");
    @summaries = $summarizer->summ_each("/directory/glob/*");



=head1 DESCRIPTION

This module allows you to summarize bodies of text into a scored hash of  I<sentences>,  I<phrase-fragments>, and  I<individual words> from the provided text.

These scores reflect the weight (precedence) of the relative text-fragments, i.e. how well they summarize or reflect the overall nature of the text. This scoring is determined through a variety of factors, including frequency distribution, clustering, and sentence-structure.

N.B.
All of the sentences are drawn from within the existing text, and are NOT proceedurally generated.



=head1 ATTRIBUTES

=head2 read-write accessible

B< The following constructor attributes are available to the user, and can be accessed/modified at any time via the form

    $summarizer->articles_path(...);

=over 8

=item C<articles_path> – C< [glob] >

glob pointing to a folder containing some text-files you wish to summarize (only necessary if you wish to bulk-process files).

defaults to 'articles/*'

=item C<permanent_path> – C< [filepath] >

file containing a non-procedural, base set of stopwords (one per line).

defaults to C< data/permanent.stop >

=item C<stopwords_path> – C< [filepath] >

file containing a procedural list of stopwords, usually generated by a Text::Summarizer::Scanner object

defaults to C< data/stopwords.stop >

=item C<print_dest> — C< [FileHandle] >

sets the output destination for the following four 'print' flags

defaults to STDOUT

=item C<print_summary> – C< [boolean] >

flag that enables visual charting of summary activity

=item C<print_graphs> – C< [boolean] >

flag that enables visual graphing of word-scoring

=item C<print_working> – C< [boolean] >

flag that enables a printout of the entire body of text currently being summarized (recommended for debugging only)

=item C<print_typifier> – C< [boolean] >

flag that enables visualization of the Text::Typifier syntactic forest

=item C<return_count> – C< [int] >

number of items to list when printing charts/graphs

=item C<phrase_thresh> – C< [int] >

minimum number of word tokens allowed in a 'phrase' unit

=item C<phrase_radius> – C< [int] >

distance iterated backward and forward from a given word when establishing a phrase (i.e. maximum length of phrase divided by 2)

=item C<freq_constant> – C< [float] >

mathematical constant for establishing minimum threshold of occurence for frequently occuring words (defaults to C<< 0.004 >>)

=back

=head2 read-only

B< These attributes are read-only, and can be accessed via the form

    $summarizer->full_text();

=over 8

=item C<stopwords> - C< [hash-ref] >

list of all stopwords loaded into the Text::Summarizer, with words as keys

=item C<article_length> — C< [int] >

number of individual words in the article (useful for statistical analysis)

=item C<full_text> – C< [string] >

all the lines of the provided text, joined together into a single continuous string

=item C<types_list> — C< [array-ref] >

complex pair-wise structure containing text-paragraphs as "keys", matched with an array containing pairs of C< type => scraps > -- best understood through L<Data::Dumper> or the like

=item C<type_scores> — C< [hash_ref] >

hash matching words with their L<Text::Typifier> scores based on usage in sentence-structure multiplied by a scaling factor for each word-type

=item C<paragraphs> — C< [array-ref] >

list containing each paragraph as separated by a L<Text::Typifier>

=item C<sentences> – C< [array-ref] >

list of each sentence found in the provided text

=item C<sen_words> – C< [array-ref] >

for each sentence, contains an array of each word in order

=item C<word_list> – C< [array-ref] >

each individual word of the entire text, in order (token stream)

=item C<freq_hash> – C< [hash-ref] >

all words that occur more than a specified threshold, paired with their frequency of occurence

=item C<clst_hash> – C< [hash-ref] >

for each word in the text, specifies the position of each occurence of the word, both relative to the sentence it occurs in and absolute within the text

=item C<phrs_hash> – C< [hash-ref] >

for each word in the text, contains a phrase of radius I<r> centered around the given word, and references the sentence from which the phrase was gathered

=item C<sigma_hash> – C< [hash-ref] >

gives the population standard deviation of the clustering of each word in the text

=item C<inter_hash> – C< [hash-ref] >

list of each chosen phrase-fragment-scrap, paired with its score

=item C<score_hash> – C< [hash-ref] >

list of each word in the text, paired with its score

=item C<phrs_list>  – C< [hash-ref] >

list of complete sentences that each scrap was drawn from, paired with its score

=item C<frag_list>  – C< [array-ref] >

for each chosen scrap, contains a hash of: the pivot word of the scrap; the sentence containing the scrap; the number of occurences of each word in the sentence; an ordered list of the words in the phrase from which the scrap was derived

=item C<file_name> – C< [string] >

the filename of the current text-source (if text was extracted from a file)

=item C<text_hint> – C< [string] >

brief snippet of text containing the first 50 and the final 30 characters of the current text

=item C<summary> – C< [hash-ref] >

scored lists of each summary sentence, each chosen scrap, and each frequently-occuring word

=back  



=head1 FUNCTIONS

=head2 C<scan>

Scan is a utility that allows the Text::Summarizer to parse through a body of text to find words that occur with unusually high frequency. These words are then stored as new stopwords via the provided C<< stopwords_path >>. Additionally, calling any of the three C<< scan_C< [...] > >> subroutines will return a reference (or array of references) to an unordered list containing the new stopwords.

      # if no argument provided, uses the 'articles_path' attribute
    $stopwords = $summarizer->scan_text( 'this is a sample text' );
    $stopwords = $summarizer->scan_file( 'some/file/path.txt' );
    @stopwords = $summarizer->scan_each( 'some/glob/*' );


=head2 C<summarize>

Summarizing is, not surprisingly, the heart of the Text::Summarizer. Summarizing a body of text provides three distinct categories of information drawn from the existing text and ordered by relevance to the summary: I<full sentences>, I<phrase-fragments / context-free token streams>, and a list of I<frequently occuring words>.

There are three provided functions for summarizing text documents.

      # if no argument provided, defaults to the 'articles_path' attribute'
    $summary   = $summarizer->summarize_text( 'this is a sample text' );
    $summary   = $summarizer->summarize_file( 'some/file/path.txt' );
    @summaries = $summarizer->summarize_each( 'some/glob/*' );

       # or their short forms

    $summary   = $summarizer->summ_text( '...' );
    $summary   = $summarizer->summ_file( '...' );
    @sumamries = $summarizer->summ_each( '...' );

C<< summarize_text >> and C<< summarize_file >> each return a summary hash-ref containing three array-refs, while C<< summarize_each >> returns a list of these hash-refs. These summary hashes take the following form:

=over 8

=item *

C<sentences> => a list of full sentences from the given text, with composite scores of the words contained therein

=item *

C<fragments> => a list of phrase fragments from the given text, scored similarly to sentences

=item *

C<words    > => a list of all words in the text, scored by a three-factor system consisting of  I<frequency of appearance>,  I<clustering>, and  I<use in important phrase fragments>.

=item *

C<typed    > => a list of all words in the text, scored syntactically (using the Text::Typifier type_factors attribute).

=item *

C<merged   > => a combination of C<words> and C<typed> scoring, adjusted by a scaling factor.

=item *

C<stemmed  > => as C<merged>, but with all words stemmed and scores merged where stemmed-forms are the same.

=back  

=head3 (note about fragments)

Phrase fragments are in actuality short "scraps" of text (usually only two or three words) that are derived from the text via the following process:

=over 8

=item 1

the entirety of the text is tokenized and scored into a C<< frequency >> table, with a high-pass threshold of frequencies above C<< # of tokens * user-defined scaling factor >>

=item 2

each sentence is tokenized and stored in an array

=item 3

for each word within the C<< frequency >> table, a table of phrase-fragments is derived by finding each occurance of said word and tracking forward and backward by a user-defined "radius" of tokens (defaults to S<C<< radius = 5 >>>, does not include the central key-word) — each phrase-fragment is thus compiled of (by default) an 11-token string

=item 4

all fragments for a given key-word are then compared to each other, and each word is deleted if it appears only once amongst all of the fragments (leaving only C<< I<A> ∪ I<B> ∪ ... ∪ I<S> >> where I<A>, I<B>, ..., I<S> are the phrase-fragments)

=item 5

what remains of each fragment is a list of "scraps" — strings of consecutive tokens — from which the longest scrap is chosen as a representation of the given phrase-fragment

=item 6

when a shorter fragment-scrap (C<I<A>>) is included in the text of a longer scrap (C<I<B>>) such that C<< I<A> ⊂ I<B> >>, the shorter is deleted and its score is added to that of the longer

=item 7

when multiple fragments are equivalent (i.e. they consist of the same list of tokens when stopwords are excluded), they are condensed into a single scrap in the form of C<< "(some|word|tokens)" >> such that the fragment now represents the tokens of the scrap (excluding stopwords) regardless of order (refered to as a "context-free token stream")

=back  



=head1 SUPPORT

Bugs should always be submitted via the project hosting bug tracker

L<https://github.com/faelin/text-summarizer/issues>

For other issues, contact the maintainer.  



=head1 AUTHOR

Faelin Landy <faelin.landy@gmail.com> (current maintainer)  



=head1 CONTRIBUTORS

* Michael McClennen <michaelm@umich.edu>  



=head1 COPYRIGHT AND LICENSE

Copyright (c) 2018 by the AUTHOR as listed above

This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.


=cut